{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "ddeda189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Рома\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'greeting_speech': ['Алло здравствуйте'],\n",
       " 'introducing_speech': ['Меня зовут ангелина компания диджитал бизнес звоню вам по поводу продления а мы сели обратила внимание что у вас срок заканчивается'],\n",
       " 'manager_name': 'Ангелина',\n",
       " 'company_name': ['компания', 'диджитал', 'бизнес'],\n",
       " 'farewall_speech': ['До свидания'],\n",
       " 'check': 'Менеджер поздоровался и попрощался'}"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Импорт необходимых библиотек\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "\n",
    "from gensim.utils import tokenize\n",
    "from natasha import (Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "    Doc\n",
    ")\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from yargy import Parser, rule\n",
    "from yargy.predicates import gram, dictionary\n",
    "\n",
    "#Загрузка русских стоп-слов\n",
    "stopwords_ru = stopwords.words(\"russian\")\n",
    "\n",
    "#Инициализация методов пакета Natasha\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "names_extractor = NamesExtractor(morph_vocab)\n",
    "\n",
    "#Загрузка данных\n",
    "dataframe=pd.read_csv('test_data.csv')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "df=dataframe.copy()\n",
    "\n",
    "def tokenize_text(raw_text: str):\n",
    "    \"Функция для токенизации текста\"\n",
    "    tokenized_str = nltk.word_tokenize(raw_text)\n",
    "    tokens = [i.lower() for i in tokenized_str if ( i not in string.punctuation )]\n",
    "    filtered_tokens = [i for i in tokens if ( i not in stopwords_ru )]\n",
    "    return filtered_tokens\n",
    "\n",
    "tokenized_text= df.text.apply(tokenize_text)\n",
    "df = df.assign(tokenized=tokenized_text)\n",
    "\n",
    "def get_manager_name(text):\n",
    "    \"Функция для получения имени менеджера\"\n",
    "    extractor = NamesExtractor(morph_vocab)\n",
    "    matches = extractor(text.title())\n",
    "    for match in matches:\n",
    "        return(match.fact.first)\n",
    "\n",
    "def get_company_name(text):\n",
    "    \"Функция для получения названия компании\"\n",
    "    company = rule (dictionary({'компания'}),gram('NOUN').repeatable())\n",
    "    parser = Parser(company)\n",
    "    for match in parser.findall(text):\n",
    "        return([x.value for x in match.tokens])\n",
    "\n",
    "#Распаковка диалогов из датасета\n",
    "dialog=[]\n",
    "for dlg in np.unique(df.dlg_id):\n",
    "    dial=''\n",
    "    for i in range(len(df[df['dlg_id']==dlg])):\n",
    "        dial+=df[df['dlg_id']==dlg].iloc[i]['role']+': - '+df[df['dlg_id']==dlg].iloc[i]['text']+' \\n '\n",
    "    dialog.append(dial)\n",
    "\n",
    "#Векторизация текста\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_text)\n",
    "# Шаблоны для векторного сравнения (не хватило времени придумать более изощренный способ. \"Это Анастасия\" - это костыль.)\n",
    "greeting_to_compare='Здравствуйте, Добрый день'\n",
    "farewall_to_compare='До свидания, всего доброго, всего хорошего'\n",
    "introducing1_to_compare='Меня зовут'\n",
    "introducing2_to_compare='Это анастасия'\n",
    "df_text_values=np.append(df.text.values,[greeting_to_compare,farewall_to_compare,introducing1_to_compare,introducing2_to_compare])\n",
    "document_matrix = vectorizer.fit_transform(df_text_values)\n",
    "\n",
    "#Расчет косинусных расстояний\n",
    "text_distance = 1-pairwise_distances(document_matrix, metric=\"cosine\")\n",
    "\n",
    "#Ранжирование по частоте индексов реплик\n",
    "greeting_sorted_similarity = np.argsort(-text_distance[480,:])\n",
    "farewall_sorted_similarity = np.argsort(-text_distance[481,:])\n",
    "introducing1_sorted_similarity = np.argsort(-text_distance[482,:])\n",
    "introducing2_sorted_similarity = np.argsort(-text_distance[483,:])\n",
    "\n",
    "#Получение индексов\n",
    "greeting_idx=list(greeting_sorted_similarity[1:12])\n",
    "farewall_idx=list(farewall_sorted_similarity[1:8])\n",
    "introducing_idx=list(introducing1_sorted_similarity[1:5])+list(introducing2_sorted_similarity[1:2])\n",
    "\n",
    "#Создание колонок с флагами\n",
    "df['greeting']=0\n",
    "df['farewall']=0\n",
    "df['introducing']=0\n",
    "df.loc[greeting_idx,'greeting']=1\n",
    "df.loc[farewall_idx,'farewall']=1\n",
    "df.loc[introducing_idx,'introducing']=1\n",
    "\n",
    "#Получение результата парсинга диалогов в виде словарей result_dlg_i\n",
    "for dlg in np.unique(df.dlg_id):\n",
    "    globals()['result_dlg_'+str(dlg)]={}\n",
    "    globals()['result_dlg_'+str(dlg)]['greeting_speech']=list(df[(df.dlg_id==dlg)&(df.role=='manager')&(df.greeting==1)].text.values)\n",
    "    globals()['result_dlg_'+str(dlg)]['introducing_speech']=list(df[(df.dlg_id==dlg)&(df.role=='manager')&(df.introducing==1)].text.values)\n",
    "    \n",
    "    globals()['result_dlg_'+str(dlg)]['manager_name']=get_manager_name(str(df[(df.dlg_id==dlg)&(df.role=='manager')&(df.introducing==1)].text.values))\n",
    "    globals()['result_dlg_'+str(dlg)]['company_name']=get_company_name(str(df[(df.dlg_id==dlg)&(df.role=='manager')&(df.introducing==1)].text.values))\n",
    "    globals()['result_dlg_'+str(dlg)]['farewall_speech']=list(df[(df.dlg_id==dlg)&(df.role=='manager')&(df.farewall==1)].text.values)\n",
    "    if ((df[(df.dlg_id==1)&(df.role=='manager')].greeting.sum()>0)&(df[(df.dlg_id==1)&(df.role=='manager')].farewall.sum()>0)):\n",
    "        globals()['result_dlg_'+str(dlg)]['check']='Менеджер поздоровался и попрощался'\n",
    "    elif ((df[(df.dlg_id==1)&(df.role=='manager')].greeting.sum()==0)&(df[(df.dlg_id==1)&(df.role=='manager')].farewall.sum()>0)):\n",
    "        globals()['result_dlg_'+str(dlg)]['check']='Менеджер поздоровался, но не попрощался'\n",
    "    elif ((df[(df.dlg_id==1)&(df.role=='manager')].greeting.sum()>0)&(df[(df.dlg_id==1)&(df.role=='manager')].farewall.sum()==0)):\n",
    "        globals()['result_dlg_'+str(dlg)]['check']='Менеджер попрощался, но не поздоровался'     \n",
    "    elif ((df[(df.dlg_id==1)&(df.role=='manager')].greeting.sum()==0)&(df[(df.dlg_id==1)&(df.role=='manager')].farewall.sum()==0)):\n",
    "        globals()['result_dlg_'+str(dlg)]['check']='Менеджер не поздоровался и не попрощался (бука)'\n",
    "\n",
    "result_dlg_1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
